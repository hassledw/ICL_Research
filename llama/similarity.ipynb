{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/hassledw/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import pickle\n",
    "import torch\n",
    "# https://huggingface.co/KaiLv\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import time\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import sys\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantical Embeddings\n",
    "\n",
    "1. https://huggingface.co/sentence-transformers\n",
    "2. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks https://arxiv.org/pdf/1908.10084.pdf\n",
    "3. https://www.sbert.net/docs/pretrained_models.html#model-overview\n",
    "4. https://www.sbert.net/examples/applications/semantic-search/README.html#semantic-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384)\n",
      "(1, 384)\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sentences we want to encode. Example:\n",
    "sentence = ['This framework generates embeddings for each input sentence']\n",
    "sentence2 = ['Bears are Really Cool']\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embedding = embedder.encode(sentence)\n",
    "embedding2 = embedder.encode(sentence2)\n",
    "print(embedding.shape)\n",
    "print(embedding2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_arrays(datasetname=\"KaiLv/UDR_Yelp\"):\n",
    "    '''\n",
    "    Creates the X_* and y_* arrays.\n",
    "    '''\n",
    "    dataset = load_dataset(datasetname)\n",
    "    X_train = np.array(dataset[\"train\"][\"question\"])\n",
    "    y_train = np.array(dataset[\"train\"][\"label\"])\n",
    "    X_test = np.array(dataset[\"test\"][\"question\"])\n",
    "\n",
    "    if datasetname == \"KaiLv/UDR_ComE\":\n",
    "        X_train = np.array([value[80:] for value in X_train])\n",
    "        X_train = np.array([value.replace(\" Options:\", \"\\nOptions:\") for value in X_train])\n",
    "        X_train = np.array([value.replace(\" A.\", \"\\nA.\") for value in X_train])\n",
    "        X_train = np.array([value.replace(\" B.\", \"\\nB.\") for value in X_train])\n",
    "        X_train = np.array([value.replace(\" C.\", \"\\nC.\") for value in X_train])\n",
    "        y_train = np.array([value[0] for value in y_train])\n",
    "        X_test = np.array([value[80:] for value in X_test])\n",
    "        X_test = np.array([value.replace(\" Options:\", \"\\nOptions:\") for value in X_test])\n",
    "        X_test = np.array([value.replace(\" A.\", \"\\nA.\") for value in X_test])\n",
    "        X_test = np.array([value.replace(\" B.\", \"\\nB.\") for value in X_test])\n",
    "        X_test = np.array([value.replace(\" C.\", \"\\nC.\") for value in X_test])\n",
    "    # y_test = np.array(dataset[\"test\"][\"label\"])\n",
    "\n",
    "    # X_debug = np.array(dataset[\"debug\"][\"sentence\"])\n",
    "    # y_debug = np.array(dataset[\"debug\"][\"label\"])\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the UDR_Yelp Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" He poured orange juice on his cereal.\\nOptions:\\nA. Orange juice is usually bright orange.\\nB. Orange juice doesn't taste good on cereal.\\nC. Orange juice is sticky if you spill it on the table.\"\n",
      " ' He drinks apple.\\nOptions:\\nA. Apple juice are very tasty and milk too\\nB. Apple can not be drunk\\nC. Apple cannot eat a human'\n",
      " ' Jeff ran 100,000 miles today\\nOptions:\\nA. 100,000 miles is way to long for one person to be able to run in one day.\\nB. Jeff is a four letter name and 100,000 has six numerical digest\\nC. 100,000 miles is longer than 100,000 km.'\n",
      " ...\n",
      " \" Harry went to the barbershop to have his glasses repaired\\nOptions:\\nA. a barbershop usually don't provide the service of repairing glasses\\nB. a barbershop usually repairs computers instead of glasses\\nC. the barbershop lacked the necessary tools to repair his glasses\"\n",
      " ' Reilly is sleeping on the window\\nOptions:\\nA. the window is open and a person cannot lay on it\\nB. the window is too cold to sleep on it\\nC. a person cannot sleep on a window'\n",
      " ' I have a desk on my lamp\\nOptions:\\nA. the lamp is made of glass that is fragile\\nB. the lamp is of poor quality\\nC. a desk is too big to be put on a lamp']\n"
     ]
    }
   ],
   "source": [
    "# Corpus with example sentences\n",
    "X_train, y_train, X_test = create_data_arrays(datasetname=\"KaiLv/UDR_ComE\")\n",
    "print(X_train)\n",
    "corpus = X_train\n",
    "queries = X_test\n",
    "\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True).to(device)\n",
    "corpus_embeddings = util.normalize_embeddings(corpus_embeddings)\n",
    "\n",
    "query_embeddings = embedder.encode(queries, convert_to_tensor=True).to(device)\n",
    "query_embeddings = util.normalize_embeddings(query_embeddings)\n",
    "\n",
    "hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=3, score_function=util.dot_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Top-K Examples for Every Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " He loves to stroll at the park with his bed\n",
      "Options:\n",
      "A. A bed is too heavy to carry with when strolling at a park\n",
      "B. walking at a park is good for health\n",
      "C. Some beds are big while some are smaller\n",
      "\t1.  a plane is on his bed\n",
      "Options:\n",
      "A. a toy plane is on the bed\n",
      "B. a plane is too large to park on bed\n",
      "C. there are beds in some plane = B\n",
      "\t2.  he usually goes to the gym to sleep\n",
      "Options:\n",
      "A. there is no bed in the gym\n",
      "B. people seldom do that\n",
      "C. The admission fee at the gym is too expensive = B\n",
      "\t3.  I bought some beds to open a cafe\n",
      "Options:\n",
      "A. beds are too expensive\n",
      "B. sitting on the bed is uncomfortable\n",
      "C. the cafe is not a place to sleep = C\n",
      " The inverter was able to power the continent.\n",
      "Options:\n",
      "A. An inverter is smaller than a car\n",
      "B. An inverter is incapable of powering an entire continent.\n",
      "C. An inverter is rechargeable.\n",
      "\t1.  Air can power cars.\n",
      "Options:\n",
      "A. Air can't be burned.\n",
      "B. Air contains no energy.\n",
      "C. Car can't run with air. = C\n",
      "\t2.  Atlantic is the biggest continent in the world\n",
      "Options:\n",
      "A. Atlantic is not big enough\n",
      "B. Asia is a continent instead of an ocean\n",
      "C. Atlantic is an ocean instead of a continent = C\n",
      "\t3.  Arizona is in Europe\n",
      "Options:\n",
      "A. North America is a continent\n",
      "B. Arizona is hot\n",
      "C. Arizona is in North America = C\n",
      " The chef put extra lemons on the pizza.\n",
      "Options:\n",
      "A. Many types of lemons are to sour to eat.\n",
      "B. Lemons and pizzas are both usually round.\n",
      "C. Lemons are not a pizza topping.\n",
      "\t1.  he made pizza with the mixer\n",
      "Options:\n",
      "A. pizza is usually round flat while mixer is of different shape\n",
      "B. Mixer cannot make solid food\n",
      "C. mixer is made of plastic and steel while pizza is made of organic foods = B\n",
      "\t2.  Pizza is usually eaten by chopsticks\n",
      "Options:\n",
      "A. Pizza is a western food and westerners don't use chopsticks\n",
      "B. Many people have trouble in using chopsticks\n",
      "C. Pizza is usually eaten by spoon = A\n",
      "\t3.  I often eat pizza to keep healthy\n",
      "Options:\n",
      "A. Healthy people don't like pizza\n",
      "B. Pizza contains a lot of fat which is not healthy\n",
      "C. The pizza tastes good = B\n"
     ]
    }
   ],
   "source": [
    "hits = np.array(hits)\n",
    "for i, query in enumerate(hits[:3]):\n",
    "    print(X_test[i])\n",
    "    for x, top_k_entry in enumerate(query):\n",
    "        train_idx = top_k_entry[\"corpus_id\"]\n",
    "        print(f\"\\t{x + 1}. {X_train[train_idx]} = {y_train[train_idx]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./token.txt\") as f:\n",
    "    token = f.readline()\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token)\n",
    "    model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(prompt, max_tokens=20):\n",
    "    '''\n",
    "    Queries the llama model.\n",
    "    '''\n",
    "    inputs = tokenizer(\n",
    "        f\"{prompt}\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "    generation_config = transformers.GenerationConfig(\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        top_k=1,\n",
    "        repetition_penalty=1.5,\n",
    "        max_new_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=torch.ones_like(input_ids),\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "\n",
    "    output_text = tokenizer.decode(\n",
    "        generation_output[0].cuda(), skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"sentence\", \"label\"])\n",
    "count = 1\n",
    "hits = np.array(hits)\n",
    "\n",
    "for i, entry in enumerate(hits):\n",
    "    query = X_test[i]\n",
    "    train_idx_1 = entry[0][\"corpus_id\"]\n",
    "    train_idx_2 = entry[1][\"corpus_id\"]\n",
    "    train_idx_3 = entry[2][\"corpus_id\"]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Here are some examples of my task:\n",
    "    1. {X_train[train_idx_1]} Response: {y_train[train_idx_1]}\n",
    "    2. {X_train[train_idx_2]} Response: {y_train[train_idx_2]}\n",
    "    3. {X_train[train_idx_3]} Response: {y_train[train_idx_3]}\n",
    "    rate the sentiment of the below review: \"very negative\", \"negative\", \"neutral\", \"positive\", or \"very positive\".\n",
    "    ###\\\"{query}\\\" Response: \"\"\"\n",
    "\n",
    "    output_text = query_model(prompt)\n",
    "\n",
    "    print(output_text)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(f\"Finished {count}/{len(X_test)}\\n\")\n",
    "    entry = [query, output_text]\n",
    "    df_entry = pd.DataFrame(entry, index=['sentence', 'label']).T\n",
    "    df = pd.concat((df, df_entry))\n",
    "    count+=1\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "df.to_csv(f\"/home/grads/hassledw/ICL_Research/UDR_yelp_results/UDR-yelp-fewshot-llama.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Result Class to Run Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 8 \"None\" entries\n",
      "Dropped 145 \"None\" entries\n",
      "Llama-7b Prediction Accuracy (Zero-shot): 47.76%\n",
      "Llama-7b Prediction Accuracy (Few-shot): 54.65%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3048256/1365130721.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"label\"] = df[\"label\"].astype(int)\n",
      "/tmp/ipykernel_3048256/1365130721.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"label\"] = df[\"label\"].astype(int)\n"
     ]
    }
   ],
   "source": [
    "class YelpResults:\n",
    "    def __init__(self, zero_yelp_df, few_yelp_df):\n",
    "        self.zero_yelp_df = zero_yelp_df\n",
    "        self.few_yelp_df = few_yelp_df\n",
    "        self.truth_df = pd.read_csv(\"/home/grads/hassledw/ICL_Research/UDR_Yelp_results/UDR-yelp-llama.csv\")\n",
    "    \n",
    "    def clean_data(self, df):\n",
    "        '''\n",
    "        Cleans the data by retrieving the label and dropping None entries. \n",
    "        '''\n",
    "        def get_response_yelp(text):\n",
    "            '''\n",
    "            Cleans the text of the label to just get the response\n",
    "            '''\n",
    "            valid = [\"very negative\", \"very positive\", \"negative\", \"positive\", \"neutral\"]\n",
    "            valid_dict = {\"very negative\": 0, \"negative\": 1, \"neutral\": 2, \"positive\": 3, \"very positive\": 4}\n",
    "            sentences = text.split(\"Response:\")\n",
    "            query = sentences[-1].strip(\"##\").strip(\" \").lower()\n",
    "            \n",
    "            if len(sentences[-1].split(\" \")) > 3 or query not in valid:\n",
    "                for v in valid:\n",
    "                    if v in query:\n",
    "                        return valid_dict[v]\n",
    "                return None\n",
    "            else:\n",
    "                return valid_dict[query]\n",
    "            \n",
    "        df[\"label\"] = df[\"label\"].apply(get_response_yelp)\n",
    "        orig_entries = df.shape[0]\n",
    "        df = df.dropna()\n",
    "        print(f\"Dropped {orig_entries - df.shape[0]} \\\"None\\\" entries\")\n",
    "        df[\"label\"] = df[\"label\"].astype(int)\n",
    "        return df\n",
    "    \n",
    "    def gather_yelp_accuracy(self, df):\n",
    "        '''\n",
    "        Gets the overall accuracy of df.\n",
    "        ''' \n",
    "        df_results = pd.merge(self.truth_df, df, on=['sentence'], how='inner')\n",
    "        accurate_results = df_results[df_results[\"label_x\"] == df_results[\"label_y\"]]\n",
    "        return len(accurate_results) / len(df_results) * 100\n",
    "    \n",
    "    def run_results(self):\n",
    "        '''\n",
    "        Runs the results of the Yelp dataset. \n",
    "        '''\n",
    "        self.zero_yelp_df = self.clean_data(self.zero_yelp_df)\n",
    "        self.few_yelp_df = self.clean_data(self.few_yelp_df)\n",
    "        print(f\"Llama-7b Prediction Accuracy (Zero-shot): {self.gather_yelp_accuracy(self.zero_yelp_df):.2f}%\")\n",
    "        print(f\"Llama-7b Prediction Accuracy (Few-shot): {self.gather_yelp_accuracy(self.few_yelp_df):.2f}%\")\n",
    "        print()\n",
    "\n",
    "test_yelp_df = pd.read_csv(\"/home/grads/hassledw/ICL_Research/UDR_Yelp_results/UDR-yelp-llama.csv\")\n",
    "zero_yelp_df = pd.read_csv(\"/home/grads/hassledw/ICL_Research/UDR_Yelp_results/UDR_Yelp-zeroshot-llama.csv\")\n",
    "few_yelp_df = pd.read_csv(\"/home/grads/hassledw/ICL_Research/UDR_Yelp_results/UDR_Yelp-fewshot-llama.csv\")\n",
    "yelpres = YelpResults(zero_yelp_df, few_yelp_df)\n",
    "yelpres.run_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"KaiLv/UDR_SNLI\")\n",
    "# data = dataset[\"test\"]\n",
    "\n",
    "# data = pd.DataFrame(data).drop(columns=[\"idx\"])\n",
    "# data.to_csv(\"/home/grads/hassledw/ICL_Research/UDR_SNLI_results/UDR-snli-llama.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 \"None\" entries\n",
      "Dropped 0 \"None\" entries\n",
      "Llama-7b Prediction Accuracy (Zero-shot): 41.80%\n",
      "Llama-7b Prediction Accuracy (Few-shot): 46.80%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SNLIResults:\n",
    "    def __init__(self, zero_snli_df, few_snli_df):\n",
    "        self.zero_snli_df = zero_snli_df\n",
    "        self.few_snli_df = few_snli_df\n",
    "        self.truth_df = pd.read_csv(\"/home/grads/hassledw/ICL_Research/UDR_SNLI_results/UDR-snli-llama.csv\")\n",
    "    \n",
    "    def clean_data(self, df):\n",
    "        '''\n",
    "        Cleans the data by retrieving the label and dropping None entries. \n",
    "        '''\n",
    "        def get_response_snli(text):\n",
    "            '''\n",
    "            Cleans the text of the label to just get the response\n",
    "            '''\n",
    "            valid = [\"entail\", \"inco\", \"contra\", \"in con\"]\n",
    "            valid_dict = {\"entailment\": 0, \"inconclusive\": 1, \"contradiction\": 2}\n",
    "\n",
    "            sentences = text.split(\"Response:\")\n",
    "            query = sentences[-1].strip(\"##\").strip(\" \").lower()\n",
    "            \n",
    "            if query not in valid:\n",
    "                for i, v in enumerate(valid):\n",
    "                    if i == 3:\n",
    "                        return valid_dict[list(valid_dict.keys())[1]]\n",
    "                    if v in query:\n",
    "                        return valid_dict[list(valid_dict.keys())[i]]\n",
    "                print(query)\n",
    "                return None\n",
    "            else:\n",
    "                return valid_dict[query]\n",
    "            \n",
    "        df[\"label\"] = df[\"label\"].apply(get_response_snli)\n",
    "        orig_entries = df.shape[0]\n",
    "        df = df.dropna()\n",
    "        print(f\"Dropped {orig_entries - df.shape[0]} \\\"None\\\" entries\")\n",
    "        df[\"label\"] = df[\"label\"].astype(int)\n",
    "        return df\n",
    "    \n",
    "    def gather_snli_accuracy(self, df):\n",
    "        '''\n",
    "        Gets the overall accuracy of df.\n",
    "        ''' \n",
    "        df_results = pd.merge(self.truth_df, df, on=['sentence'], how='inner')\n",
    "        accurate_results = df_results[df_results[\"label_x\"] == df_results[\"label_y\"]]\n",
    "        return len(accurate_results) / len(df_results) * 100\n",
    "    \n",
    "    def run_results(self):\n",
    "        '''\n",
    "        Runs the results of the Yelp dataset. \n",
    "        '''\n",
    "        self.zero_snli_df = self.clean_data(self.zero_snli_df)\n",
    "        self.few_snli_df = self.clean_data(self.few_snli_df)\n",
    "        print(f\"Llama-7b Prediction Accuracy (Zero-shot): {self.gather_snli_accuracy(self.zero_snli_df):.2f}%\")\n",
    "        print(f\"Llama-7b Prediction Accuracy (Few-shot): {self.gather_snli_accuracy(self.few_snli_df):.2f}%\")\n",
    "        print()\n",
    "\n",
    "zero_snli_df = pd.read_csv(\"/home/grads/hassledw/ICL_Research/UDR_SNLI_results/UDR_SNLI-zeroshot-llama.csv\")\n",
    "few_snli_df = pd.read_csv(\"/home/grads/hassledw/ICL_Research/UDR_SNLI_results/UDR_SNLI-fewshot-llama.csv\")\n",
    "yelpres = SNLIResults(zero_snli_df, few_snli_df)\n",
    "yelpres.run_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
