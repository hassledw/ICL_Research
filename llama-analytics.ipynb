{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Model Analytics on Yelp Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Ground Truth CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"KaiLv/UDR_Yelp\")\n",
    "# data = dataset[\"test\"]\n",
    "\n",
    "# data = pd.DataFrame(data).drop(columns=[\"idx\"])\n",
    "# data.to_csv(\"/home/grads/hassledw/ICL_Research/UDR-yelp-llama.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Label Data in Zero-Shot and Run Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 276 \"None\" entries\n",
      "Dropped 146 \"None\" entries\n"
     ]
    }
   ],
   "source": [
    "truth_df = pd.read_csv(\"/home/grads/hassledw/ICL_Research/UDR_yelp_results/UDR-yelp-llama.csv\")\n",
    "zero_df = pd.read_csv(\"/home/grads/hassledw/ICL_Research/UDR_yelp_results/UDR-yelp-zeroshot-llama.csv\")\n",
    "few_df = pd.read_csv(\"/home/grads/hassledw/ICL_Research/UDR_yelp_results/UDR-yelp-fewshot-llama.csv\")\n",
    "\n",
    "def get_response(text):\n",
    "    '''\n",
    "    Cleans the text of the label to just get the response\n",
    "    '''\n",
    "    valid = [\"very negative\", \"very positive\", \"negative\", \"positive\", \"neutral\"]\n",
    "    valid_dict = {\"very negative\": 0, \"negative\": 1, \"neutral\": 2, \"positive\": 3, \"very positive\": 4}\n",
    "    sentences = text.split(\"Response:\")\n",
    "    query = sentences[-1].strip(\"##\").strip(\" \").lower()\n",
    "    \n",
    "    if len(sentences[-1].split(\" \")) > 3 or query not in valid:\n",
    "        for v in valid:\n",
    "            if v in query:\n",
    "                return valid_dict[v]\n",
    "        return None\n",
    "    else:\n",
    "        return valid_dict[query]\n",
    "    \n",
    "\n",
    "zero_df[\"label\"] = zero_df[\"label\"].apply(get_response)\n",
    "few_df[\"label\"] = few_df[\"label\"].apply(get_response)\n",
    "\n",
    "orig_entries = zero_df.shape[0]\n",
    "orig_entries_few = few_df.shape[0]\n",
    "\n",
    "zero_df = zero_df.dropna()\n",
    "few_df = few_df.dropna()\n",
    "\n",
    "print(f\"Dropped {orig_entries - zero_df.shape[0]} \\\"None\\\" entries\")\n",
    "print(f\"Dropped {orig_entries_few - few_df.shape[0]} \\\"None\\\" entries\")\n",
    "\n",
    "zero_df[\"label\"] = zero_df[\"label\"].astype(int)\n",
    "few_df[\"label\"] = few_df[\"label\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_df.to_csv(\"/home/grads/hassledw/ICL_Research/UDR_yelp_results/UDR-yelp-zeroshot-llama-cleaned.csv\")\n",
    "few_df.to_csv(\"/home/grads/hassledw/ICL_Research/UDR_yelp_results/UDR-yelp-fewshot-llama-cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Ground Truth and Zero-Shot\n",
    "Here are the rrompts used for the baseline tasks.\n",
    "\n",
    "**Zero Shot**:\n",
    "```\n",
    "Please rate the sentiment of the following text # \"very negative\", \"negative\", \"neutral\", \"positive\", or \"very positive\"#:\n",
    "\"### \\\"{query}\\\"\"\n",
    "\"### Response:\"\"\"\n",
    "```\n",
    "\n",
    "**Few Shot**:\n",
    "\n",
    "```\n",
    "Here are some demonstration examples for the sentiment classification task:\n",
    "    1. \\\"{similar_dict[(value + 1) % len(X_test)][0][:200]}...\\\" = \\\"{similar_dict[(value + 1) % len(X_test)][1]}\\\"\n",
    "    2. \\\"{similar_dict[(value + 2) % len(X_test)][0][:200]}...\\\" = \\\"{similar_dict[(value + 2) % len(X_test)][1]}\\\"\n",
    "Please rate the sentiment of the following text # \"very negative\", \"negative\", \"neutral\", \"positive\", or \"very positive\"#.\n",
    "\"### \\\"{query}\\\"\"\n",
    "\"### Response:\"\"\"\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-7b Prediction Accuracy (Zero-shot): 42.00%\n",
      "Llama-7b Prediction Accuracy (Few-shot): 44.57%\n"
     ]
    }
   ],
   "source": [
    "zero_results = pd.merge(truth_df, zero_df, on=['sentence'], how='inner').drop(columns=[\"Unnamed: 0_x\", \"Unnamed: 0.1\", \"Unnamed: 0_y\"])\n",
    "few_results = pd.merge(truth_df, few_df, on=['sentence'], how='inner')\n",
    "\n",
    "\n",
    "accurate_results_zero = zero_results[zero_results[\"label_x\"] == zero_results[\"label_y\"]]\n",
    "accurate_results_few = few_results[few_results[\"label_x\"] == few_results[\"label_y\"]]\n",
    "\n",
    "print(f\"Llama-7b Prediction Accuracy (Zero-shot): {len(accurate_results_zero) / len(zero_results) * 100:.2f}%\")\n",
    "print(f\"Llama-7b Prediction Accuracy (Few-shot): {len(accurate_results_few) / len(few_results) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
